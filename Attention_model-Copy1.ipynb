{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we will encode a positive review as a 1 and a negative review as a 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-11T18:22:05.182558Z",
     "start_time": "2019-05-11T18:22:03.556467Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-11T18:22:05.651810Z",
     "start_time": "2019-05-11T18:22:05.649214Z"
    }
   },
   "outputs": [],
   "source": [
    "TRAIN_PATH = 'aclImdb/train'\n",
    "TEST_PATH = 'aclImdb/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-11T18:22:06.571087Z",
     "start_time": "2019-05-11T18:22:06.565950Z"
    }
   },
   "outputs": [],
   "source": [
    "def data_read(path, train_or_test, pos_or_neg):\n",
    "    reviews, labels = [], []\n",
    "    \n",
    "    for filename in os.listdir(path + '/' + pos_or_neg):\n",
    "        with open(path + '/' + pos_or_neg + '/' + filename) as f:\n",
    "            reviews.append(f.read())\n",
    "        if pos_or_neg == 'pos':\n",
    "            label = 1\n",
    "        else:\n",
    "            label = 0\n",
    "        labels.append(label)\n",
    "    \n",
    "    return reviews, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-11T18:22:15.592921Z",
     "start_time": "2019-05-11T18:22:07.415570Z"
    }
   },
   "outputs": [],
   "source": [
    "pos_results = data_read(TRAIN_PATH, 'train', 'pos')\n",
    "reviews = pos_results[0]\n",
    "labels = pos_results[1]\n",
    "\n",
    "neg_results = data_read(TRAIN_PATH, 'train', 'neg')\n",
    "reviews += neg_results[0]\n",
    "labels += neg_results[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-11T18:22:15.613709Z",
     "start_time": "2019-05-11T18:22:15.594804Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame({'reviews': reviews, 'y': labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-11T18:22:15.632562Z",
     "start_time": "2019-05-11T18:22:15.615682Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>For a movie that gets no respect there sure ar...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bizarre horror movie filled with famous faces ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A solid, if unremarkable film. Matthau, as Ein...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It's a strange feeling to sit alone in a theat...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You probably all already know this by now, but...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             reviews  y\n",
       "0  For a movie that gets no respect there sure ar...  1\n",
       "1  Bizarre horror movie filled with famous faces ...  1\n",
       "2  A solid, if unremarkable film. Matthau, as Ein...  1\n",
       "3  It's a strange feeling to sit alone in a theat...  1\n",
       "4  You probably all already know this by now, but...  1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-11T18:22:28.671090Z",
     "start_time": "2019-05-11T18:22:27.698010Z"
    }
   },
   "outputs": [],
   "source": [
    "pos_results = data_read(TRAIN_PATH, 'test', 'pos')\n",
    "reviews = pos_results[0]\n",
    "labels = pos_results[1]\n",
    "\n",
    "neg_results = data_read(TRAIN_PATH, 'test', 'neg')\n",
    "reviews += neg_results[0]\n",
    "labels += neg_results[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-11T18:23:14.511105Z",
     "start_time": "2019-05-11T18:23:14.493564Z"
    }
   },
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame({'reviews': reviews, 'y': labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-11T18:23:15.326429Z",
     "start_time": "2019-05-11T18:23:15.316325Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>For a movie that gets no respect there sure ar...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bizarre horror movie filled with famous faces ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A solid, if unremarkable film. Matthau, as Ein...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It's a strange feeling to sit alone in a theat...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You probably all already know this by now, but...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             reviews  y\n",
       "0  For a movie that gets no respect there sure ar...  1\n",
       "1  Bizarre horror movie filled with famous faces ...  1\n",
       "2  A solid, if unremarkable film. Matthau, as Ein...  1\n",
       "3  It's a strange feeling to sit alone in a theat...  1\n",
       "4  You probably all already know this by now, but...  1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-11T18:23:16.590462Z",
     "start_time": "2019-05-11T18:23:16.582810Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>My comments may be a bit of a spoiler, for wha...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>The \"saucy\" misadventures of four au pairs who...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>Oh, those Italians! Assuming that movies about...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>Eight academy nominations? It's beyond belief....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>Not that I dislike childrens movies, but this ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 reviews  y\n",
       "24995  My comments may be a bit of a spoiler, for wha...  0\n",
       "24996  The \"saucy\" misadventures of four au pairs who...  0\n",
       "24997  Oh, those Italians! Assuming that movies about...  0\n",
       "24998  Eight academy nominations? It's beyond belief....  0\n",
       "24999  Not that I dislike childrens movies, but this ...  0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-11T18:24:50.236817Z",
     "start_time": "2019-05-11T18:23:19.104005Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df['reviews'] = [msg.lower() for msg in train_df['reviews']]\n",
    "train_df['reviews'] = [s.translate(str.maketrans('', '', string.punctuation)) for s in train_df['reviews']]\n",
    "train_df['reviews'] = [' '.join([ps.stem(w) for w in msg.split()]) for msg in train_df['reviews']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-11T18:26:18.305224Z",
     "start_time": "2019-05-11T18:24:50.238883Z"
    }
   },
   "outputs": [],
   "source": [
    "test_df['reviews'] = [msg.lower() for msg in test_df['reviews']]\n",
    "test_df['reviews'] = [s.translate(str.maketrans('', '', string.punctuation)) for s in test_df['reviews']]\n",
    "test_df['reviews'] = [' '.join([ps.stem(w) for w in msg.split()]) for msg in test_df['reviews']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-11T18:26:18.309679Z",
     "start_time": "2019-05-11T18:26:18.306991Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 15\n",
    "\n",
    "UNK_token = 0\n",
    "PAD_token = 1\n",
    "SOS_token = 2\n",
    "EOS_token = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-11T18:26:18.322837Z",
     "start_time": "2019-05-11T18:26:18.311941Z"
    }
   },
   "outputs": [],
   "source": [
    "class Voc(Dataset):\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.trimmed = False\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: 'PAD', SOS_token: 'SOS', \n",
    "                           EOS_token: 'EOS', UNK_token: 'UNK'}\n",
    "        self.num_words = 4 # include the ones above\n",
    "\n",
    "    def add_sentence(self, sentence):\n",
    "        for word in sentence.split():\n",
    "            self.add_word(word.lower())\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.num_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.num_words] = word\n",
    "            self.num_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "    # remove words that appear less frequently\n",
    "    def trim(self, min_count):\n",
    "        if self.trimmed:\n",
    "            return\n",
    "        self.trimmed = True\n",
    "        keep_words = []\n",
    "        for k, v in self.word2count.items():\n",
    "            if v >= min_count:\n",
    "                keep_words.append(k)\n",
    "\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: 'PAD', SOS_token: 'SOS', \n",
    "                           EOS_token: 'EOS', UNK_token: 'UNK'}\n",
    "        self.num_words = 4\n",
    "        for word in keep_words:\n",
    "            self.add_word(word)\n",
    "\n",
    "# takes string sentence and returns sentence of word indices\n",
    "def indexesFromSentence(voc, sentence):\n",
    "    return [voc.word2index[word] for word in sentence.split()] + [EOS_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-11T18:26:18.329599Z",
     "start_time": "2019-05-11T18:26:18.324999Z"
    }
   },
   "outputs": [],
   "source": [
    "class ReviewDataset():\n",
    "    def __init__(self, df):\n",
    "        \"\"\"Bad coding practice to use the globals but it will work for now\"\"\"\n",
    "        self.reviews = df\n",
    "        self.text = df['reviews']\n",
    "        self.rating = df['y']\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (self.text[idx], self.rating[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-11T18:26:18.334944Z",
     "start_time": "2019-05-11T18:26:18.331948Z"
    }
   },
   "outputs": [],
   "source": [
    "reviews = Voc('reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-11T18:26:21.495804Z",
     "start_time": "2019-05-11T18:26:18.337336Z"
    }
   },
   "outputs": [],
   "source": [
    "for review in train_df['reviews']:\n",
    "    reviews.add_sentence(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-11T18:26:21.501153Z",
     "start_time": "2019-05-11T18:26:21.498443Z"
    }
   },
   "outputs": [],
   "source": [
    "train_ds = ReviewDataset(train_df)\n",
    "test_ds = ReviewDataset(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-11T18:26:21.514037Z",
     "start_time": "2019-05-11T18:26:21.503343Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1161, 493, 3]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexesFromSentence(reviews, 'hello world')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write out reviews to pickle object for later prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-30T17:37:57.148669Z",
     "start_time": "2019-04-30T17:37:57.108633Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(reviews, open('./model/reviews.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-30T17:37:57.154467Z",
     "start_time": "2019-04-30T17:37:57.151431Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_ds,\n",
    "                              batch_size=1000,\n",
    "                              shuffle=True, \n",
    "                              num_workers=4)\n",
    "\n",
    "test_dataloader = DataLoader(test_ds,\n",
    "                             batch_size=1000,\n",
    "                             shuffle=True,\n",
    "                             num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Han model is going to fit a GRU, and for each word i in the sequence, we will collect the hidden layer $h_{i}$. Then for the attention model we have the following algorithm, for word $w_{t}$ in sentence of length T."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$u_{t} = tanh(W_{w}h_{t} + b_{w})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\alpha_{t} = \\frac{exp(u_{t}^{T}u_{w})}{\\Sigma_{t}exp(u_{t}^{T}u_{w})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$s = \\Sigma_{t} \\alpha_{t}h_{t}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "s is then input into a neural network for regression or classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-30T17:37:57.166079Z",
     "start_time": "2019-04-30T17:37:57.156824Z"
    }
   },
   "outputs": [],
   "source": [
    "class HAN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=60, hidden_size=15, num_layers=1, dropout=0.1):\n",
    "        super(HAN, self).__init__()\n",
    "        self.gru = nn.GRU(input_size=embedding_dim, hidden_size=hidden_size,\n",
    "                          num_layers=num_layers, batch_first=True,\n",
    "                              dropout=(0 if num_layers==1 else dropout),\n",
    "                              bidirectional=True)\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.fc1 = nn.Linear(2*hidden_size, 100)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.fc2 = nn.Linear(100, 1, bias=False)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.fc3 = nn.Linear(2*hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, input_seq, input_lengths, hidden=None):\n",
    "        # convert word indices to embeddings\n",
    "        embedded = self.embedding(input_seq)\n",
    "        \n",
    "        # pass embeddings through GRU\n",
    "        # all_layers is shape batch_size x max(sentence_length) x hidden_size*2\n",
    "        all_layers, final = self.gru(embedded, hidden)\n",
    "\n",
    "        # pass the hidden layers through linear layer\n",
    "        # each word of each document will now be a length 100 tensor\n",
    "        u = self.tanh( self.fc1(all_layers) )\n",
    "        \n",
    "        # map the length 100 tensor to a scalar representing importance\n",
    "        # take the softmax to get the word importance WRT the document\n",
    "        alpha = self.softmax( self.fc2(u) )\n",
    "        \n",
    "        # take sum of hidden layers for each sentence weighted by the alphas \n",
    "        s = (all_layers*alpha).sum(dim=1)\n",
    "        \n",
    "        # take the linear combination of hidden layers and \n",
    "        # plug into Linear Layer and Sigmoid to get probability\n",
    "        s = self.sigmoid( self.fc3(s) )\n",
    "        \n",
    "        return s.squeeze(), alpha.squeeze() # will add alphas for visualization later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-30T17:37:57.181893Z",
     "start_time": "2019-04-30T17:37:57.169155Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        losses = []\n",
    "        \n",
    "        for i, mails in enumerate(train_dataloader):\n",
    "            # read in values from dataloader\n",
    "            x = mails[0]\n",
    "            y = mails[1]\n",
    "\n",
    "            # index sentence and get lengths\n",
    "            x = [sen.split() for sen in x]\n",
    "            lengths = [len(sen) for sen in x]\n",
    "            x = [[reviews.word2index.get(word, UNK_token) for word in sentence] for sentence in x]\n",
    "\n",
    "            # create padded matrix\n",
    "            batch_size = len(y)\n",
    "            max_length = max(lengths)\n",
    "            padded_x = np.ones((batch_size, max_length))*PAD_token\n",
    "            for idx, x_len in enumerate(lengths):\n",
    "                sequence = x[idx]\n",
    "                padded_x[idx, 0:x_len] = sequence[:x_len]\n",
    "\n",
    "            x = torch.Tensor(padded_x).long()\n",
    "            y_hat, alpha = han(x, lengths)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            loss = criterion(y_hat, y.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses += [loss.item()]\n",
    "            \n",
    "        print(f'Train Loss after {epoch+1} epochs: {np.mean(losses)}')\n",
    "        if epoch % 5 == 4:\n",
    "            test(model)\n",
    "\n",
    "\n",
    "def test(model):\n",
    "    model.eval()\n",
    "    losses, correct, total = [], 0, 0\n",
    "    for i, mails in enumerate(test_dataloader):\n",
    "        # read in values from dataloader\n",
    "        x = mails[0]\n",
    "        y = mails[1]\n",
    "\n",
    "        # index sentence and get lengths\n",
    "        x = [sen.split() for sen in x]\n",
    "        lengths = [len(sen) for sen in x]\n",
    "        x = [[reviews.word2index.get(word, UNK_token) for word in sentence] for sentence in x]\n",
    "\n",
    "        # create padded matrix\n",
    "        batch_size = len(y)\n",
    "        max_length = max(lengths)\n",
    "        \n",
    "        padded_x = np.ones((batch_size, max_length))*PAD_token\n",
    "        for idx, x_len in enumerate(lengths):\n",
    "            sequence = x[idx]\n",
    "            padded_x[idx, 0:x_len] = sequence[:x_len]\n",
    "\n",
    "        x = torch.Tensor(padded_x).long()\n",
    "        y_hat, _ = han(x, lengths)\n",
    "\n",
    "        # cross-entropy\n",
    "        loss = criterion(y_hat, y.float())\n",
    "        losses += [loss.item()]\n",
    "        \n",
    "        # accuracy\n",
    "        # print(y_hat == y)\n",
    "        # correct += sum( y_hat == y )\n",
    "        # total += y.shape.squeeze().tolist()\n",
    "        \n",
    "    print(f'Test Loss: {np.mean(losses)}\\n')\n",
    "    # print(f'Test Accuracy: {correct / total}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-30T21:46:32.885245Z",
     "start_time": "2019-04-30T17:37:57.184468Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss after 1 epochs: 0.6950127649307251\n",
      "Train Loss after 2 epochs: 0.6808984565734864\n",
      "Train Loss after 3 epochs: 0.6182800579071045\n",
      "Train Loss after 4 epochs: 0.5813623476028442\n",
      "Train Loss after 5 epochs: 0.5632908606529236\n",
      "Test Loss: 0.5549108505249023\n",
      "\n",
      "Train Loss after 6 epochs: 0.5505723786354065\n",
      "Train Loss after 7 epochs: 0.5451652956008911\n",
      "Train Loss after 8 epochs: 0.5390304303169251\n",
      "Train Loss after 9 epochs: 0.5345871043205261\n",
      "Train Loss after 10 epochs: 0.5307740354537964\n",
      "Test Loss: 0.5264209222793579\n",
      "\n",
      "Train Loss after 11 epochs: 0.5287617301940918\n",
      "Train Loss after 12 epochs: 0.524873719215393\n",
      "Train Loss after 13 epochs: 0.5228457617759704\n",
      "Train Loss after 14 epochs: 0.5233321905136108\n",
      "Train Loss after 15 epochs: 0.5254122829437256\n",
      "Test Loss: 0.5217818689346313\n",
      "\n",
      "Train Loss after 16 epochs: 0.522824981212616\n",
      "Train Loss after 17 epochs: 0.5209297347068786\n",
      "Train Loss after 18 epochs: 0.5194813179969787\n",
      "Train Loss after 19 epochs: 0.51853999376297\n",
      "Train Loss after 20 epochs: 0.5173629522323608\n",
      "Test Loss: 0.5177715706825257\n",
      "\n"
     ]
    }
   ],
   "source": [
    "3V = len(reviews.index2word)\n",
    "han = HAN(hidden_size=40, vocab_size=V, embedding_dim=50)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(han.parameters(), lr=0.01)\n",
    "\n",
    "train(han, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-30T21:46:32.994985Z",
     "start_time": "2019-04-30T21:46:32.916535Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(han.state_dict(), '/Users/jon_ross/Desktop/HAN_clean.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resume here by loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-29T15:57:13.104314Z",
     "start_time": "2019-04-29T15:57:12.892311Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HAN(\n",
       "  (gru): GRU(50, 40, batch_first=True, bidirectional=True)\n",
       "  (embedding): Embedding(251641, 50)\n",
       "  (fc1): Linear(in_features=80, out_features=100, bias=True)\n",
       "  (tanh): Tanh()\n",
       "  (fc2): Linear(in_features=100, out_features=1, bias=False)\n",
       "  (softmax): Softmax()\n",
       "  (fc3): Linear(in_features=80, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V = len(reviews.index2word)\n",
    "han = HAN(hidden_size=40, vocab_size=V, embedding_dim=50)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(han.parameters(), lr=0.01)\n",
    "\n",
    "han.load_state_dict(torch.load('/Users/jon_ross/sideProj/website/model/HAN_lower.pt'))\n",
    "han.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-21T19:23:59.076581Z",
     "start_time": "2019-04-21T17:19:49.880107Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss after 1 epochs: 0.5214558720588685\n",
      "Train Loss after 2 epochs: 0.5156813716888428\n",
      "Train Loss after 3 epochs: 0.514751056432724\n",
      "Train Loss after 4 epochs: 0.5144688582420349\n",
      "Train Loss after 5 epochs: 0.5147349214553834\n",
      "Test Loss: 0.5148994946479797\n",
      "\n",
      "Train Loss after 6 epochs: 0.5157215976715088\n",
      "Train Loss after 7 epochs: 0.5156684958934784\n",
      "Train Loss after 8 epochs: 0.5150830388069153\n",
      "Train Loss after 9 epochs: 0.5154467701911927\n",
      "Train Loss after 10 epochs: 0.5136247301101684\n",
      "Test Loss: 0.5127641570568084\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train(han, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-21T23:39:23.157536Z",
     "start_time": "2019-04-21T21:41:07.364210Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss after 1 epochs: 0.5129567635059357\n",
      "Train Loss after 2 epochs: 0.512871778011322\n",
      "Train Loss after 3 epochs: 0.5124013948440552\n",
      "Train Loss after 4 epochs: 0.5126206040382385\n",
      "Train Loss after 5 epochs: 0.512900333404541\n",
      "Test Loss: 0.5128588724136353\n",
      "\n",
      "Train Loss after 6 epochs: 0.512196718454361\n",
      "Train Loss after 7 epochs: 0.512180050611496\n",
      "Train Loss after 8 epochs: 0.5134016942977905\n",
      "Train Loss after 9 epochs: 0.5119988882541656\n",
      "Train Loss after 10 epochs: 0.5118386626243592\n",
      "Test Loss: 0.5111083388328552\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train(han, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T02:36:43.939007Z",
     "start_time": "2019-04-21T23:47:29.077146Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss after 1 epochs: 0.5113566875457763\n",
      "Train Loss after 2 epochs: 0.5108390665054321\n",
      "Train Loss after 3 epochs: 0.5110727632045746\n",
      "Train Loss after 4 epochs: 0.5109731912612915\n",
      "Train Loss after 5 epochs: 0.5106906700134277\n",
      "Test Loss: 0.5103581595420837\n",
      "\n",
      "Train Loss after 6 epochs: 0.5105051362514496\n",
      "Train Loss after 7 epochs: 0.5103676033020019\n",
      "Train Loss after 8 epochs: 0.5101380777359009\n",
      "Train Loss after 9 epochs: 0.5100115621089936\n",
      "Train Loss after 10 epochs: 0.5100088846683503\n",
      "Test Loss: 0.5098927581310272\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train(han, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T06:09:47.331929Z",
     "start_time": "2019-04-22T03:35:07.362093Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss after 1 epochs: 0.5100021982192993\n",
      "Train Loss after 2 epochs: 0.5100687432289124\n",
      "Train Loss after 3 epochs: 0.5098020458221435\n",
      "Train Loss after 4 epochs: 0.5097366654872895\n",
      "Train Loss after 5 epochs: 0.5096450424194336\n",
      "Test Loss: 0.5096076893806457\n",
      "\n",
      "Train Loss after 6 epochs: 0.5096347463130951\n",
      "Train Loss after 7 epochs: 0.5096398425102234\n",
      "Train Loss after 8 epochs: 0.5097169744968414\n",
      "Train Loss after 9 epochs: 0.5099311256408692\n",
      "Train Loss after 10 epochs: 0.5104057645797729\n",
      "Test Loss: 0.5098689675331116\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train(han, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T09:36:57.998776Z",
     "start_time": "2019-04-26T05:47:42.067533Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss after 1 epochs: 0.5106182849407196\n",
      "Train Loss after 2 epochs: 0.5109199261665345\n",
      "Train Loss after 3 epochs: 0.5100345838069916\n",
      "Train Loss after 4 epochs: 0.5095074379444122\n",
      "Train Loss after 5 epochs: 0.5095135581493377\n",
      "Test Loss: 0.5092803609371185\n",
      "\n",
      "Train Loss after 6 epochs: 0.5093630003929138\n",
      "Train Loss after 7 epochs: 0.5092614543437958\n",
      "Train Loss after 8 epochs: 0.5091621339321136\n",
      "Train Loss after 9 epochs: 0.5092394518852233\n",
      "Train Loss after 10 epochs: 0.5098104321956635\n",
      "Test Loss: 0.5089573621749878\n",
      "\n",
      "Train Loss after 11 epochs: 0.5090854740142823\n",
      "Train Loss after 12 epochs: 0.5092040395736694\n",
      "Train Loss after 13 epochs: 0.5090126204490661\n",
      "Train Loss after 14 epochs: 0.5087879073619842\n",
      "Train Loss after 15 epochs: 0.5083959913253784\n",
      "Test Loss: 0.5081046855449677\n",
      "\n",
      "Train Loss after 16 epochs: 0.5083082234859466\n",
      "Train Loss after 17 epochs: 0.5081970369815827\n",
      "Train Loss after 18 epochs: 0.5079572975635529\n",
      "Train Loss after 19 epochs: 0.5079389655590058\n",
      "Train Loss after 20 epochs: 0.507836765050888\n",
      "Test Loss: 0.5078209745883941\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train(han, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-28T22:15:29.456880Z",
     "start_time": "2019-04-28T20:14:09.631017Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss after 1 epochs: 0.507928593158722\n",
      "Train Loss after 2 epochs: 0.5085471534729004\n",
      "Train Loss after 3 epochs: 0.5081615054607391\n",
      "Train Loss after 4 epochs: 0.5079267120361328\n",
      "Train Loss after 5 epochs: 0.5077857565879822\n",
      "Test Loss: 0.5076735424995422\n",
      "\n",
      "Train Loss after 6 epochs: 0.50793212890625\n",
      "Train Loss after 7 epochs: 0.5078068602085114\n",
      "Train Loss after 8 epochs: 0.5076055216789246\n",
      "Train Loss after 9 epochs: 0.507716772556305\n",
      "Train Loss after 10 epochs: 0.5075769853591919\n",
      "Test Loss: 0.5078012835979462\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train(han, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-29T19:56:59.229093Z",
     "start_time": "2019-04-29T15:57:29.330912Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss after 1 epochs: 0.5077660274505615\n",
      "Train Loss after 2 epochs: 0.5077108120918274\n",
      "Train Loss after 3 epochs: 0.5076938283443451\n",
      "Train Loss after 4 epochs: 0.508044751882553\n",
      "Train Loss after 5 epochs: 0.5074990367889405\n",
      "Test Loss: 0.5074690568447113\n",
      "\n",
      "Train Loss after 6 epochs: 0.507408915758133\n",
      "Train Loss after 7 epochs: 0.507298446893692\n",
      "Train Loss after 8 epochs: 0.5072609877586365\n",
      "Train Loss after 9 epochs: 0.5072231805324554\n",
      "Train Loss after 10 epochs: 0.5071840131282807\n",
      "Test Loss: 0.5071391451358795\n",
      "\n",
      "Train Loss after 11 epochs: 0.5071390974521637\n",
      "Train Loss after 12 epochs: 0.5071385967731475\n",
      "Train Loss after 13 epochs: 0.5071383833885192\n",
      "Train Loss after 14 epochs: 0.5071383047103882\n",
      "Train Loss after 15 epochs: 0.5071383035182953\n",
      "Test Loss: 0.5071379935741425\n",
      "\n",
      "Train Loss after 16 epochs: 0.5071378803253174\n",
      "Train Loss after 17 epochs: 0.5071383500099182\n",
      "Train Loss after 18 epochs: 0.5071239829063415\n",
      "Train Loss after 19 epochs: 0.5071846306324005\n",
      "Train Loss after 20 epochs: 0.507312821149826\n",
      "Test Loss: 0.507386714220047\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train(han, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-29T19:56:59.427409Z",
     "start_time": "2019-04-29T19:56:59.262202Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(han.state_dict(), '/Users/jon_ross/Desktop/HAN_lower.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-18T21:01:30.894111Z",
     "start_time": "2019-04-18T21:01:30.637646Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import transforms\n",
    "\n",
    "def text_color(x, y, strings, colors, ax=None, **kw): \n",
    "    \"\"\"x and y will be equal to 0\n",
    "    strings is a list of words\n",
    "    colors is a list of numbers [0,1] \n",
    "    where 1 represents the most important word\"\"\"\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    t = ax.transData\n",
    "    canvas = ax.figure.canvas\n",
    "\n",
    "    # horizontal version\n",
    "    for s, c in zip(strings, colors):\n",
    "        text = ax.text(x, y, s + \" \", color=(0.69-0.69*c,0.69-0.69*c,1), transform=t, **kw)\n",
    "        text.draw(canvas.get_renderer())\n",
    "        ex = text.get_window_extent()\n",
    "        t = transforms.offset_copy(\n",
    "            text.get_transform(), x=ex.width, units='dots')\n",
    "        \n",
    "\n",
    "def visualize_attention(sentence):\n",
    "    sentence_tensor = torch.Tensor( [reviews.word2index[word] for word in sentence.split()] ).long()\n",
    "    sentence_tensor = sentence_tensor.unsqueeze(dim=0)\n",
    "    \n",
    "    p, alpha = han( sentence_tensor, [len(sentence.split())] )\n",
    "    print(f'Probability of positive review: {p}')\n",
    "    \n",
    "    text_color(0, 0, sentence.split(), alpha.tolist())\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Unseen words are breaking the visualize function and I think it's because of the casing issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-17T00:04:55.679503Z",
     "start_time": "2019-04-17T00:04:55.545449Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of positive review: 0.0004007560492027551\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_attention('this film was horrible and i hated it')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-17T00:05:10.431533Z",
     "start_time": "2019-04-17T00:05:07.417378Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of positive review: 0.9995682835578918\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_attention('wonderful story about a family')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-17T00:05:21.450455Z",
     "start_time": "2019-04-17T00:05:21.361141Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of positive review: 0.9996213912963867\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_attention('fun for the whole family')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "notify_time": "5"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
